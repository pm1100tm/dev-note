# 🚀 Elastic Cache 전략

AWS 캐싱 구현 시 고려사항이 있습니다.

- 이 데이터를 캐시해도 안전한가?
- 이 데이터를 캐싱하면 효과가 있는가?
- 데이터 구조가 캐싱하기에 적절한가?
- 어떤 캐싱 패턴을 적용할 것인가?
- Cache Evictions and TTL(Time-to-Live)

---

## 🧩 이 데이터를 캐시해도 안전한가?

⚠️ 핵심 이슈: 데이터 불일치 / 최신성(Consistency)

- 캐시는 데이터의 복사본을 보관하기 때문에 원본 데이터가 변경되어도 캐시가 바로 갱신되지 않을 수 있음.
  - → 즉, “out-of-date”, “eventually consistent”

✅ 캐싱이 안전한 경우

- 데이터가 자주 바뀌지 않음: 상품 정보, 카테고리, 코드 목록
- 약간의 지연 허용 가능: 조회성 데이터, 뉴스 목록, 인기글 랭킹
- 읽기(READ)가 압도적으로 많음: 대부분 SELECT 위주 서비스

❌ 캐싱이 위험한 경우

- 데이터가 자주 변함: 실시간 재고, 주가, 실시간 알림
- 변경 즉시 반영되어야 함: 결제 상태, 포인트 잔액
- 업데이트 실패 시 문제 발생: 은행 계좌, 주문 처리

## 🧩 이 데이터를 캐싱하면 효과가 있는가?

캐시의 목적은 “자주 요청되는 데이터를 빠르게 제공”하는 것이므로, 데이터 특성상 캐시
적중률(Cache Hit Ratio)이 높을수록 효과적입니다.

✅ 캐싱이 효과적인 데이터

- 변경 빈도 낮고, 조회 빈도 높은 데이터
  - 예: 상품 상세, 회원 프로필, 랭킹 페이지
- 일부 Key만 자주 호출되는 데이터
  - 예: 특정 인기 상품, 상위 10개 게시글 등

❌ 비효율적인 캐싱 데이터

- 데이터 전체가 자주 바뀌는 경우
  - 예: 실시간 센서 데이터, 주식 호가
- 모든 키가 균등하게 호출되는 경우
  - 예: 매번 다른 사용자 ID, 랜덤 값 요청 등
    → 캐시 적중률이 낮음 → 오히려 메모리 낭비

## 🧩 데이터 구조가 캐싱하기에 적절한가?

Redis, Memcached는 Key-Value 구조가 기본이므로 데이터가 단일 Key로 접근 가능해야 효과적입니다.

- Key-Value 캐싱: "user:1001" → {name:"홍길동", age:35}
- 집계 결과 캐싱: "report:monthly_sales_2025_11" → 48293
- 페이지 캐싱: "home:top_posts" → [post_id_1, post_id_2, ...]

## 🤔 어떤 캐싱 패턴을 적용할 것인가?

| 패턴                           | 동작 방식                                                 | 특징                                  |
| ------------------------------ | --------------------------------------------------------- | ------------------------------------- |
| **Cache-Aside (Lazy Loading)** | 앱이 DB 조회 전 캐시 확인 → 없으면 DB 조회 후 캐시에 저장 | 가장 일반적, 단순                     |
| **Write-Through**              | DB에 쓸 때 캐시에도 동시에 저장                           | 항상 최신 데이터 유지, 쓰기 부하 증가 |
| **Write-Behind (Write-Back)**  | 캐시에 먼저 쓰고, 비동기로 DB에 반영                      | 빠른 쓰기, 데이터 유실 위험           |
| **Read-Through**               | 캐시가 직접 DB 조회 (Cache가 DB 프록시 역할)              | 캐시 계층이 로직 담당                 |
| **TTL (Time-To-Live)**         | 일정 시간 후 캐시 자동 만료                               | 데이터 최신성 보장에 유용             |

> ✅ **가장 일반적인 조합: → Cache-Aside + TTL 설정**

---

### 🧶 Cache-Aside (Lazy Loading / Lazy Population)

> “데이터를 요청할 때만 캐시에 적재한다.”

```shell
[사용자 요청]
   ↓
① 캐시 조회
   ↓ (miss)
② DB 조회
   ↓
③ 캐시에 저장
   ↓
④ 결과 반환
```

✅ 장점 (Pros)

- 필요한 데이터만 캐싱: 실제 요청된 데이터만 저장 → 불필요한 메모리 낭비 없음
- 노드 장애에도 복구 쉬움: 캐시가 사라져도 DB에서 다시 불러오면 됨 (Warm-up 자동)

⚠️ 단점 (Cons)

- Cache Miss Penalty: 캐시에 없을 때는 DB → 캐시 → 클라이언트로 3단계 왕복 (지연 발생)
- Stale Data (데이터 불일치): DB가 업데이트되었는데 캐시 갱신이 안 되면 오래된 데이터가 남을 수 있음

💡 보완 전략

- TTL (Time-To-Live) 설정으로 일정 시간마다 캐시 자동 갱신
- Cache Invalidation 로직으로 DB 변경 시 캐시 삭제

### 🧶 Write-Through Caching

> “DB에 데이터를 쓰는 순간, 캐시도 함께 업데이트한다.”

즉, DB를 갱신할 때 항상 캐시도 갱신해서 항상 최신 데이터를 유지하도록 만드는 방식.

```shell
[데이터 갱신 요청]
   ↓
① DB에 Write
   ↓
② 캐시에도 동일한 Write
```

✅ 장점 (Pros)

- ⚡ 항상 최신 상태 유지: 캐시가 DB와 항상 동기화됨
- 🚀 빠른 Read 성능: 데이터가 이미 캐시에 반영되어 있어 즉시 응답 가능

⚠️ 단점 (Cons)

- 🐢 쓰기 비용 증가 (Write Penalty): 매번 DB + 캐시 2곳에 쓰기 발생
- 📉 Cache Churn (낭비): 한 번도 읽히지 않는 데이터도 캐시에 들어감
- ⚠️ 초기 Missing Data 문제: DB에 데이터 추가 전엔 캐시에 없음 → Lazy Loading과 병행 필요

---

### Cache-Aside VS Write-Through Caching

| 항목                | Cache-Aside (Lazy Loading)       | Write-Through                   |
| ------------------- | -------------------------------- | ------------------------------- |
| **캐시 채움 시점**  | 데이터 요청 시 (on-demand)       | DB 갱신 시 (자동)               |
| **읽기 성능**       | 첫 요청은 느림 (cache miss)      | 항상 빠름                       |
| **쓰기 성능**       | 빠름 (캐시 쓰기 없음)            | 느림 (DB+Cache 두 번)           |
| **데이터 최신성**   | 낮음 (stale risk)                | 높음 (항상 최신)                |
| **캐시 낭비**       | 없음 (필요한 데이터만)           | 있음 (안 쓰이는 데이터도 저장)  |
| **복원성**          | 높음 (노드 장애 시 자동 복구)    | 높음 (항상 최신 데이터 유지)    |
| **구현 난이도**     | 쉬움                             | 복잡 (DB와 캐시 동기화 필요)    |
| **적합한 시나리오** | 읽기 중심, 데이터 변경 적은 경우 | 쓰기 중심, 최신성이 중요한 경우 |

<br>

---

## 🧩 혼합 전략: Write-Through + Lazy Loading

AWS에서 자주 추천하는 전략입니다 👇

> 🌈 Write-Through로 캐시를 항상 최신으로 유지하면서, 캐시에 존재하지 않는 데이터는 Lazy
> Loading(Cache-Aside) 으로 보완.

즉 👇

- 새로운 데이터 조회 시 → Lazy Loading으로 캐시 채움
- DB 갱신 시 → 캐시도 즉시 업데이트

이 조합은 성능 + 일관성 사이의 밸런스를 잘 맞추는 패턴입니다.

| 구성 요소                         | 역할                                     |
| --------------------------------- | ---------------------------------------- |
| **Amazon RDS / DynamoDB**         | 원본 데이터 저장소                       |
| **Amazon ElastiCache (Redis)**    | 캐시 계층                                |
| **Application (예: Spring Boot)** | Cache-Aside 또는 Write-Through 로직 구현 |
| **TTL**                           | 데이터 만료 관리                         |
| **CloudWatch**                    | Cache Hit/Miss 비율 모니터링             |

### 혼합 전략으로 한 사이클 돌려보기

- 🧠 전제 조건
  - 서비스: 사용자 상세 조회 API
    - /api/users/{userId}
  - 원본 데이터: RDS (user table)
  - 캐시 계층: ElastiCache (Redis)
  - 캐시 키 예시: user:USER_123

#### 1️⃣ [요청 #1] — 처음으로 사용자 상세정보를 요청 (Cache Miss 발생)

클라이언트가 처음으로 GET /api/users/USER_123 요청을 보냅니다.

- → 캐시에는 아직 데이터가 없음 (Cache Miss)

```shell
① Redis 조회 → ❌ 없음
② RDS 조회 → ✅ user_123 정보 조회
③ Redis에 저장 (key=user:USER_123, TTL=5분)
④ 클라이언트로 응답 반환
```

💬 결과:

- 캐시가 자동으로 “채워짐(populated)”
- 이후 같은 요청은 캐시에서 바로 응답 가능

#### 2️⃣ [요청 #2~#N] — 같은 사용자 상세정보 반복 조회 (Cache Hit 발생)

다른 사용자나 같은 브라우저가 GET /api/users/USER_123 다시 요청

```shell
① Redis 조회 → ✅ 존재
② Redis에서 즉시 반환 (RDS 미조회)
```

💬 결과:

- DB 조회 생략 → 초고속 응답
- DB 부하 감소

#### 3️⃣ [사용자 정보 수정] — 이름 변경 (Write-Through 동작)

사용자가 이름을 수정함 → PUT /api/users/USER_123

```shell
① RDS 업데이트 (name="홍길순")
② 동일 데이터를 Redis에도 즉시 업데이트
    → key=user:USER_123 값 갱신
```

💬 결과:

- DB와 캐시가 동시에 최신화
- 이후 모든 조회 요청이 항상 최신 데이터 반환

💬 결과:

- DB와 캐시가 동시에 최신화
- 이후 모든 조회 요청이 항상 최신 데이터 반환

#### 4️⃣ [추가 요청] — 수정된 사용자 상세 조회

이제 다른 인스턴스나 사용자가 다시 조회 요청

- → Redis에 최신 데이터가 이미 들어있음

```shell
① Redis 조회 → ✅ 최신 데이터 존재
② Redis에서 즉시 반환
③ RDS 접근 불필요
```

💬 결과:

- 최신 데이터 유지 (일관성 ✅)
- 빠른 응답 속도 (성능 ✅)

#### 5️⃣ [데이터 만료 또는 장애] — 캐시 TTL 만료 or 노드 재시작 (Lazy Loading 복구)

Redis의 TTL(예: 5분)이 지나거나 노드 장애로 캐시 데이터가 사라짐

```shell
① Redis 조회 → ❌ (만료 또는 삭제)
② RDS 조회 → ✅ 최신 데이터 조회
③ Redis에 다시 저장 (Lazy Loading)
```

💬 결과:

- 캐시가 자동으로 “Warm-up”
- 별도의 복구 작업 없이 정상 작동

---

## ⚙️ Cache Evictions and TTL (Time-to-Live)

- 캐시는 데이터를 메모리에 임시 저장하는 계층입니다.
- 하지만 메모리는 유한하기 때문에, 언젠가는 저장된 데이터를 비워(evict) 줘야 합니다.

### Cache Eviction (캐시 제거)가 발생하는 3가지 경우

- 🧹 1. 명시적 삭제 (Manual Eviction)
  - 애플리케이션 코드에서 직접 캐시 항목을 삭제
  - cache.evict("user:123") or DEL user:123
- ♻️ 2. 메모리 부족으로 인한 자동 제거 (LRU Eviction)
  - 캐시 메모리가 꽉 차면, 가장 최근에 사용되지 않은 항목부터 삭제
  - Redis의 LRU(Least Recently Used) 알고리즘
- ⏰ 3. TTL(Time-To-Live) 만료
  - 아이템에 설정된 만료 시간이 지나면 자동 삭제
  - SET key value EX 3600 → 1시간 뒤 자동 삭제

### Eviction 정책 (Redis 기준)

Redis는 메모리가 꽉 찼을 때 어떤 데이터를 지울지 정책을 선택할 수 있습니다.

| 정책 이름           | 설명                                            |
| ------------------- | ----------------------------------------------- |
| **noeviction**      | 메모리 초과 시 오류 반환 (기본값)               |
| **allkeys-lru**     | 모든 키 중 가장 오래 사용되지 않은 항목 삭제    |
| **volatile-lru**    | TTL이 설정된 키 중 오래 사용되지 않은 항목 삭제 |
| **allkeys-random**  | 임의의 키 삭제                                  |
| **volatile-ttl**    | TTL이 가장 짧게 남은 키 삭제                    |
| **volatile-random** | TTL 설정된 키 중 임의로 삭제                    |

> 💡 일반적으로 운영 환경에서는
>
> → volatile-lru 또는 allkeys-lru 가 가장 많이 사용됩니다.

### 📉 Eviction이 자주 발생할 때의 대처법

| 문제                                   | 해결 방법                                                             |
| -------------------------------------- | --------------------------------------------------------------------- |
| **메모리 부족으로 자주 Eviction 발생** | ✅ Scale Up (더 큰 캐시 노드로)<br>✅ Scale Out (Cluster 모드로 확장) |
| **캐시가 너무 빨리 지워짐**            | TTL 연장 또는 LRU 정책 변경                                           |
| **특정 키가 너무 커서 메모리 낭비**    | Key 설계 개선 (chunking, hash 구조 활용)                              |

---

💡 한 줄 요약

- TTL(Time-To-Live) 은 캐시 데이터의 “유통기한”이고,
- Eviction 은 메모리가 꽉 찼을 때 “오래된 데이터부터 내보내는” 메커니즘입니다.

TTL은 데이터 신선도를, Eviction은 시스템 안정성을 유지하기 위한 필수 장치입니다.
